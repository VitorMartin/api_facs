{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "908db170-2219-4a4c-9dc0-e0c6ef83cfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.utils import img_to_array\n",
    "from keras.utils.image_utils import smart_resize\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f'Python: {sys.version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffdac63b-febf-47ca-b258-878a7402c262",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a26b555-4003-4294-891f-a69ef5066c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with pre trained model \n",
    "\n",
    "#base_model = MobileNet(input_shape=(224, 224, 3), include_top=False)\n",
    "base_model = MobileNet(\n",
    "    input_shape=None,\n",
    "    alpha=1.0,\n",
    "    depth_multiplier=1,\n",
    "    dropout=0.001,\n",
    "    include_top=True,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    pooling=None,\n",
    "    classes=1000,\n",
    "    classifier_activation=\"softmax\"\n",
    ")\n",
    "\n",
    "\n",
    "for layer in base_model.layers:\n",
    "  layer.trainable = False\n",
    "\n",
    "\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(units=2 , activation='softmax')(x)\n",
    "\n",
    "# creating our model.\n",
    "model = Model(base_model.input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2616ac31-9f8e-485c-9395-85d719fdb47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=categorical_crossentropy , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d63a896a-3533-44d5-84f0-ed6b8681b3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = []\n",
    "directory_template = './data/images/images/train/$session$/'\n",
    "for session in os.listdir('./data/images/images/train'):\n",
    "    # Find XML filename\n",
    "    directory = directory_template.replace('$session$', session)\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    for file in files:\n",
    "        # Load JPG filename\n",
    "        jpg_filename = directory + file\n",
    "        \n",
    "        emotion_dict = {\n",
    "            'emotion': session,\n",
    "            'img': jpg_filename\n",
    "        }\n",
    "        sessions.append(emotion_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5630f906-232f-476c-8eba-9b848f83dceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of           emotion                                           img  aus\n",
       "session                                                             \n",
       "0           angry        ./data/images/images/train/angry/0.jpg    0\n",
       "1           angry        ./data/images/images/train/angry/1.jpg    0\n",
       "2           angry       ./data/images/images/train/angry/10.jpg    0\n",
       "3           angry    ./data/images/images/train/angry/10002.jpg    0\n",
       "4           angry    ./data/images/images/train/angry/10016.jpg    0\n",
       "...           ...                                           ...  ...\n",
       "28816    surprise  ./data/images/images/train/surprise/9969.jpg    0\n",
       "28817    surprise  ./data/images/images/train/surprise/9985.jpg    0\n",
       "28818    surprise  ./data/images/images/train/surprise/9990.jpg    0\n",
       "28819    surprise  ./data/images/images/train/surprise/9992.jpg    0\n",
       "28820    surprise  ./data/images/images/train/surprise/9996.jpg    0\n",
       "\n",
       "[28821 rows x 3 columns]>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sessions = pd.DataFrame(sessions)\n",
    "df_sessions.index.rename('session', inplace=True)\n",
    "df_sessions[\"aus\"] = 0\n",
    "df_sessions.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9974fbc-f848-42f7-bdc2-e1b1d0e2e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_aus = pd.read_excel('data/aus.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9323b6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anger = df_sessions.loc[lambda df: df['emotion'] == \"angry\"].reset_index()\n",
    "df_disgust = df_sessions.loc[lambda df: df['emotion'] == \"disgust\"].reset_index()\n",
    "df_happy = df_sessions.loc[lambda df: df['emotion'] == \"happy\"].reset_index()\n",
    "df_fear = df_sessions.loc[lambda df: df['emotion'] == \"fear\"].reset_index()\n",
    "df_surprise = df_sessions.loc[lambda df: df['emotion'] == \"surprise\"].reset_index()\n",
    "df_sadness = df_sessions.loc[lambda df: df['emotion'] == \"sad\"].reset_index()\n",
    "df_neutral = df_sessions.loc[lambda df: df['emotion'] == \"neutral\"].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7c45982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rodol\\AppData\\Local\\Temp\\ipykernel_18744\\3835781970.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sadness[\"aus\"][i] = random_num\n",
      "C:\\Users\\rodol\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "list_anger = [\"4,5,7,17,23\", \"4,5,7,17,24\"]\n",
    "list_disgust = [\"9,17\", \"10, 17\"]\n",
    "list_sadness = [\"1,4,15\", \"1,4,15,17\"]\n",
    "list_fear = [\"1,2,4,5\",\"1,2,4,5,25\"]\n",
    "list_surprise=[\"1,2,5,26\", \"1,2,26\"]\n",
    "list_happy = [\"6,12\"]\n",
    "list_neutral = [\"0\"]\n",
    "\n",
    "for i in range(len(df_sadness.values)):\n",
    "     rand_idx = random.randrange(len(list_sadness))\n",
    "     random_num = list_sadness[rand_idx]\n",
    "\n",
    "     df_sadness[\"aus\"][i] = random_num\n",
    "\n",
    "# i = 0\n",
    "# for i in range(len(df_fear.values)):\n",
    "#     rand_idx = random.randrange(len(list_fear))\n",
    "#     random_num = list_fear[rand_idx]\n",
    "\n",
    "#     df_fear[\"aus\"][i] = random_num\n",
    "\n",
    "# i = 0\n",
    "# for i in range(len(df_surprise.values)):\n",
    "#      rand_idx = random.randrange(len(list_surprise))\n",
    "#      random_num = list_surprise[rand_idx]\n",
    "\n",
    "#      df_surprise[\"aus\"][i] = random_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2eb00a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sadness.to_csv(\"base_aus_sadness.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "367b8f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelo = pd.read_csv(\"base_aus_sadness.csv\")\n",
    "df_modelo = df_modelo.drop(columns=[\"Unnamed: 0\", \"session\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d29c105c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>img</th>\n",
       "      <th>aus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>sad</td>\n",
       "      <td>./data/images/images/train/sad/25557.jpg</td>\n",
       "      <td>1,4,15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>sad</td>\n",
       "      <td>./data/images/images/train/sad/13883.jpg</td>\n",
       "      <td>1,4,15,17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2288</th>\n",
       "      <td>sad</td>\n",
       "      <td>./data/images/images/train/sad/25073.jpg</td>\n",
       "      <td>1,4,15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>sad</td>\n",
       "      <td>./data/images/images/train/sad/15411.jpg</td>\n",
       "      <td>1,4,15,17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3671</th>\n",
       "      <td>sad</td>\n",
       "      <td>./data/images/images/train/sad/33848.jpg</td>\n",
       "      <td>1,4,15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3206</th>\n",
       "      <td>sad</td>\n",
       "      <td>./data/images/images/train/sad/31156.jpg</td>\n",
       "      <td>1,4,15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>sad</td>\n",
       "      <td>./data/images/images/train/sad/11073.jpg</td>\n",
       "      <td>1,4,15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>sad</td>\n",
       "      <td>./data/images/images/train/sad/12277.jpg</td>\n",
       "      <td>1,4,15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>sad</td>\n",
       "      <td>./data/images/images/train/sad/16932.jpg</td>\n",
       "      <td>1,4,15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2737</th>\n",
       "      <td>sad</td>\n",
       "      <td>./data/images/images/train/sad/28086.jpg</td>\n",
       "      <td>1,4,15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3456 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     emotion                                       img        aus\n",
       "2371     sad  ./data/images/images/train/sad/25557.jpg     1,4,15\n",
       "628      sad  ./data/images/images/train/sad/13883.jpg  1,4,15,17\n",
       "2288     sad  ./data/images/images/train/sad/25073.jpg     1,4,15\n",
       "853      sad  ./data/images/images/train/sad/15411.jpg  1,4,15,17\n",
       "3671     sad  ./data/images/images/train/sad/33848.jpg     1,4,15\n",
       "...      ...                                       ...        ...\n",
       "3206     sad  ./data/images/images/train/sad/31156.jpg     1,4,15\n",
       "165      sad  ./data/images/images/train/sad/11073.jpg     1,4,15\n",
       "363      sad  ./data/images/images/train/sad/12277.jpg     1,4,15\n",
       "1068     sad  ./data/images/images/train/sad/16932.jpg     1,4,15\n",
       "2737     sad  ./data/images/images/train/sad/28086.jpg     1,4,15\n",
       "\n",
       "[3456 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df_modelo, test_size=0.3)\n",
    "train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6be12063-4f60-41cb-a603-01016ee67c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3456 validated image filenames belonging to 2 classes.\n",
      "Found 1482 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    zoom_range = 0.2, \n",
    "    shear_range = 0.2, \n",
    "    horizontal_flip=True, \n",
    "    rescale = 1./255\n",
    ")\n",
    "\n",
    "train_data = train_datagen.flow_from_dataframe(\n",
    "    dataframe = pd.concat([train]).sample(frac=1),\n",
    "    directory=\".\",\n",
    "    x_col = \"img\",\n",
    "    y_col = \"aus\",\n",
    "    subset = \"training\",\n",
    "    batch_size = 32,\n",
    "    seed = 42,\n",
    "    class_mode = \"categorical\",\n",
    "    target_size = (48,48)\n",
    ")\n",
    "\n",
    "test_data = train_datagen.flow_from_dataframe(\n",
    "    dataframe = pd.concat([test]).sample(frac=1),\n",
    "    directory=\".\",\n",
    "    x_col = \"img\",\n",
    "    y_col = \"aus\",\n",
    "    subset = \"training\",\n",
    "    batch_size = 32,\n",
    "    seed = 42,\n",
    "    class_mode = \"categorical\",\n",
    "    target_size = (48,48)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a6a8d0f-99a5-4b15-be3e-ff61c1014198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "10/10 [==============================] - 5s 324ms/step - loss: 0.6931 - accuracy: 0.4875 - val_loss: 0.6932 - val_accuracy: 0.4961\n",
      "Epoch 2/30\n",
      "10/10 [==============================] - 2s 237ms/step - loss: 0.6932 - accuracy: 0.5063 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 3/30\n",
      "10/10 [==============================] - 2s 184ms/step - loss: 0.6934 - accuracy: 0.4906 - val_loss: 0.6933 - val_accuracy: 0.4961\n",
      "Epoch 4/30\n",
      "10/10 [==============================] - 2s 180ms/step - loss: 0.6938 - accuracy: 0.4656 - val_loss: 0.6936 - val_accuracy: 0.4492\n",
      "Epoch 5/30\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 0.6931 - accuracy: 0.5156 - val_loss: 0.6935 - val_accuracy: 0.4531\n",
      "Epoch 6/30\n",
      "10/10 [==============================] - 2s 160ms/step - loss: 0.6932 - accuracy: 0.4969 - val_loss: 0.6932 - val_accuracy: 0.4883\n",
      "Epoch 7/30\n",
      "10/10 [==============================] - 2s 154ms/step - loss: 0.6932 - accuracy: 0.5031 - val_loss: 0.6932 - val_accuracy: 0.4531\n",
      "Epoch 8/30\n",
      "10/10 [==============================] - 2s 175ms/step - loss: 0.6932 - accuracy: 0.5344 - val_loss: 0.6941 - val_accuracy: 0.4336\n",
      "Epoch 9/30\n",
      "10/10 [==============================] - 1s 141ms/step - loss: 0.6930 - accuracy: 0.5125 - val_loss: 0.6933 - val_accuracy: 0.4922\n",
      "Epoch 10/30\n",
      "10/10 [==============================] - 1s 142ms/step - loss: 0.6929 - accuracy: 0.5281 - val_loss: 0.6925 - val_accuracy: 0.5273\n",
      "Epoch 11/30\n",
      "10/10 [==============================] - 1s 134ms/step - loss: 0.6924 - accuracy: 0.5281 - val_loss: 0.6946 - val_accuracy: 0.4570\n",
      "Epoch 12/30\n",
      "10/10 [==============================] - 1s 130ms/step - loss: 0.6926 - accuracy: 0.5219 - val_loss: 0.6943 - val_accuracy: 0.4766\n",
      "Epoch 13/30\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 0.6932 - accuracy: 0.5031 - val_loss: 0.6942 - val_accuracy: 0.4805\n",
      "Epoch 14/30\n",
      "10/10 [==============================] - 1s 150ms/step - loss: 0.6899 - accuracy: 0.5781 - val_loss: 0.6939 - val_accuracy: 0.4922\n",
      "Epoch 15/30\n",
      "10/10 [==============================] - 1s 140ms/step - loss: 0.6943 - accuracy: 0.4875 - val_loss: 0.6943 - val_accuracy: 0.4883\n",
      "Epoch 16/30\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.6950 - accuracy: 0.4750 - val_loss: 0.6943 - val_accuracy: 0.4844\n",
      "Epoch 17/30\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.6930 - accuracy: 0.5125 - val_loss: 0.6935 - val_accuracy: 0.4961\n",
      "Epoch 18/30\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.6926 - accuracy: 0.5188 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 19/30\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.6946 - accuracy: 0.4719 - val_loss: 0.6948 - val_accuracy: 0.4609\n",
      "Epoch 20/30\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.6930 - accuracy: 0.5063 - val_loss: 0.6942 - val_accuracy: 0.4727\n",
      "Epoch 21/30\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.6926 - accuracy: 0.5219 - val_loss: 0.6917 - val_accuracy: 0.5469\n",
      "Epoch 22/30\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.6938 - accuracy: 0.4875 - val_loss: 0.6945 - val_accuracy: 0.4609\n",
      "Epoch 23/30\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.6941 - accuracy: 0.4688 - val_loss: 0.6941 - val_accuracy: 0.4570\n",
      "Epoch 24/30\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.6928 - accuracy: 0.5219 - val_loss: 0.6926 - val_accuracy: 0.5352\n",
      "Epoch 25/30\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.6925 - accuracy: 0.5375 - val_loss: 0.6942 - val_accuracy: 0.4531\n",
      "Epoch 26/30\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.6941 - accuracy: 0.4625 - val_loss: 0.6945 - val_accuracy: 0.4414\n",
      "Epoch 27/30\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.6928 - accuracy: 0.5219 - val_loss: 0.6931 - val_accuracy: 0.5078\n",
      "Epoch 28/30\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 0.6928 - accuracy: 0.5219 - val_loss: 0.6926 - val_accuracy: 0.5273\n",
      "Epoch 29/30\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.6933 - accuracy: 0.4969 - val_loss: 0.6930 - val_accuracy: 0.5078\n",
      "Epoch 30/30\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.6926 - accuracy: 0.5219 - val_loss: 0.6939 - val_accuracy: 0.4727\n",
      "\n",
      "Accuracy: max_acc = 57.81%\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(\n",
    "    train_data,\n",
    "    steps_per_epoch=10,\n",
    "    epochs=30,\n",
    "    validation_data=test_data,\n",
    "    validation_steps=8,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath='./results/model_aus_sadness.h5',\n",
    "            save_best_only=True,\n",
    "            monitor=\"accuracy\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print()\n",
    "max_acc = max(hist.history['accuracy']) * 100\n",
    "print(f'Accuracy: {max_acc = :.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8626785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step\n",
      "1,4,15,17: 50.29%\n"
     ]
    }
   ],
   "source": [
    "labels = np.sort(df_modelo['aus'].unique())\n",
    "img_cv = cv.imread('./data/images/images/validation/sad/350.jpg', cv.IMREAD_COLOR)\n",
    "img_keras = img_to_array(img_cv)\n",
    "img_keras = np.expand_dims(img_keras, axis=0)\n",
    "img_keras = smart_resize(img_keras, (224, 224))\n",
    "prediction = model.predict(img_keras)[0]\n",
    "mean = prediction.mean()\n",
    "std = prediction.std()\n",
    "prediction_idx = np.argmax(prediction)\n",
    "prediction_label = labels[prediction_idx]\n",
    "prediction_prob = prediction[prediction_idx]\n",
    "\n",
    "print(f'{prediction_label}: {round(prediction_prob * 100, 2)}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "3bac574fe11901c98da7470beb09037204aaaea101993e5af58b068248c4f83d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
