{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "908db170-2219-4a4c-9dc0-e0c6ef83cfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.utils import img_to_array\n",
    "from keras.utils.image_utils import smart_resize\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f'Python: {sys.version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffdac63b-febf-47ca-b258-878a7402c262",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a26b555-4003-4294-891f-a69ef5066c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with pre trained model \n",
    "\n",
    "#base_model = MobileNet(input_shape=(224, 224, 3), include_top=False)\n",
    "base_model = MobileNet(\n",
    "    input_shape=None,\n",
    "    alpha=1.0,\n",
    "    depth_multiplier=1,\n",
    "    dropout=0.001,\n",
    "    include_top=True,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    pooling=None,\n",
    "    classes=1000,\n",
    "    classifier_activation=\"softmax\"\n",
    ")\n",
    "\n",
    "\n",
    "for layer in base_model.layers:\n",
    "  layer.trainable = False\n",
    "\n",
    "\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(units=2 , activation='softmax')(x)\n",
    "\n",
    "# creating our model.\n",
    "model = Model(base_model.input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2616ac31-9f8e-485c-9395-85d719fdb47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=categorical_crossentropy , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d63a896a-3533-44d5-84f0-ed6b8681b3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = []\n",
    "directory_template = './data/images/images/train/$session$/'\n",
    "for session in os.listdir('./data/images/images/train'):\n",
    "    # Find XML filename\n",
    "    directory = directory_template.replace('$session$', session)\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    for file in files:\n",
    "        # Load JPG filename\n",
    "        jpg_filename = directory + file\n",
    "        \n",
    "        emotion_dict = {\n",
    "            'emotion': session,\n",
    "            'img': jpg_filename\n",
    "        }\n",
    "        sessions.append(emotion_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5630f906-232f-476c-8eba-9b848f83dceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of           emotion                                           img  aus\n",
       "session                                                             \n",
       "0           angry        ./data/images/images/train/angry/0.jpg    0\n",
       "1           angry        ./data/images/images/train/angry/1.jpg    0\n",
       "2           angry       ./data/images/images/train/angry/10.jpg    0\n",
       "3           angry    ./data/images/images/train/angry/10002.jpg    0\n",
       "4           angry    ./data/images/images/train/angry/10016.jpg    0\n",
       "...           ...                                           ...  ...\n",
       "28816    surprise  ./data/images/images/train/surprise/9969.jpg    0\n",
       "28817    surprise  ./data/images/images/train/surprise/9985.jpg    0\n",
       "28818    surprise  ./data/images/images/train/surprise/9990.jpg    0\n",
       "28819    surprise  ./data/images/images/train/surprise/9992.jpg    0\n",
       "28820    surprise  ./data/images/images/train/surprise/9996.jpg    0\n",
       "\n",
       "[28821 rows x 3 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sessions = pd.DataFrame(sessions)\n",
    "df_sessions.index.rename('session', inplace=True)\n",
    "df_sessions[\"aus\"] = 0\n",
    "df_sessions.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9974fbc-f848-42f7-bdc2-e1b1d0e2e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_aus = pd.read_excel('data/aus.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9323b6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anger = df_sessions.loc[lambda df: df['emotion'] == \"angry\"].reset_index()\n",
    "df_disgust = df_sessions.loc[lambda df: df['emotion'] == \"disgust\"].reset_index()\n",
    "df_happy = df_sessions.loc[lambda df: df['emotion'] == \"happy\"].reset_index()\n",
    "df_fear = df_sessions.loc[lambda df: df['emotion'] == \"fear\"].reset_index()\n",
    "df_surprise = df_sessions.loc[lambda df: df['emotion'] == \"surprise\"].reset_index()\n",
    "df_sadness = df_sessions.loc[lambda df: df['emotion'] == \"sad\"].reset_index()\n",
    "df_neutral = df_sessions.loc[lambda df: df['emotion'] == \"neutral\"].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7c45982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rodol\\AppData\\Local\\Temp\\ipykernel_9400\\4287541825.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_fear[\"aus\"][i] = random_num\n",
      "C:\\Users\\rodol\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "list_anger = [\"4,5,7,17,23\", \"4,5,7,17,24\"]\n",
    "list_disgust = [\"9,17\", \"10, 17\"]\n",
    "list_sadness = [\"1,4,11,15,54,64\",\"1,4,15\",\"6,15,54,64\",\"11,17\",\"1,4,15,17\"]\n",
    "list_fear = [\"1,2,4,5\",\"1,2,4,5,25\"]\n",
    "list_surprise=[\"1,2,5,26\", \"1,2,26\"]\n",
    "list_happy = [\"6,12\"]\n",
    "list_neutral = [\"0\"]\n",
    "\n",
    "for i in range(len(df_fear.values)):\n",
    "     rand_idx = random.randrange(len(list_fear))\n",
    "     random_num = list_fear[rand_idx]\n",
    "\n",
    "     df_fear[\"aus\"][i] = random_num\n",
    "\n",
    "# i = 0\n",
    "# for i in range(len(df_surprise.values)):\n",
    "#      rand_idx = random.randrange(len(list_surprise))\n",
    "#      random_num = list_surprise[rand_idx]\n",
    "\n",
    "#      df_surprise[\"aus\"][i] = random_num\n",
    "\n",
    "\n",
    "# df_anger[\"aus\"] = list_anger[2]\n",
    "# df_disgust[\"aus\"] = list_disgust[0]\n",
    "# df_sadness[\"aus\"] = list_sadness[0]\n",
    "# df_fear[\"aus\"] = list_fear[0]\n",
    "# df_surprise[\"aus\"] = list_surprise[0]\n",
    "# df_happy[\"aus\"] = list_happy[0]\n",
    "# df_neutral[\"aus\" ] = list_neutral[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eb00a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_modelo = pd.concat([df_anger])\n",
    "df_fear.to_csv(\"base_aus_fear.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "367b8f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelo = pd.read_csv(\"base_aus_fear.csv\")\n",
    "df_modelo = df_modelo.drop(columns=[\"Unnamed: 0\", \"session\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d29c105c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>img</th>\n",
       "      <th>aus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>fear</td>\n",
       "      <td>./data/images/images/train/fear/23373.jpg</td>\n",
       "      <td>1,2,4,5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3441</th>\n",
       "      <td>fear</td>\n",
       "      <td>./data/images/images/train/fear/4966.jpg</td>\n",
       "      <td>1,2,4,5,25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>fear</td>\n",
       "      <td>./data/images/images/train/fear/20570.jpg</td>\n",
       "      <td>1,2,4,5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3908</th>\n",
       "      <td>fear</td>\n",
       "      <td>./data/images/images/train/fear/8541.jpg</td>\n",
       "      <td>1,2,4,5,25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3517</th>\n",
       "      <td>fear</td>\n",
       "      <td>./data/images/images/train/fear/5539.jpg</td>\n",
       "      <td>1,2,4,5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>fear</td>\n",
       "      <td>./data/images/images/train/fear/10993.jpg</td>\n",
       "      <td>1,2,4,5,25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>fear</td>\n",
       "      <td>./data/images/images/train/fear/11442.jpg</td>\n",
       "      <td>1,2,4,5,25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531</th>\n",
       "      <td>fear</td>\n",
       "      <td>./data/images/images/train/fear/21845.jpg</td>\n",
       "      <td>1,2,4,5,25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>fear</td>\n",
       "      <td>./data/images/images/train/fear/20928.jpg</td>\n",
       "      <td>1,2,4,5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>fear</td>\n",
       "      <td>./data/images/images/train/fear/16754.jpg</td>\n",
       "      <td>1,2,4,5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2872 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     emotion                                        img         aus\n",
       "1730    fear  ./data/images/images/train/fear/23373.jpg     1,2,4,5\n",
       "3441    fear   ./data/images/images/train/fear/4966.jpg  1,2,4,5,25\n",
       "1371    fear  ./data/images/images/train/fear/20570.jpg     1,2,4,5\n",
       "3908    fear   ./data/images/images/train/fear/8541.jpg  1,2,4,5,25\n",
       "3517    fear   ./data/images/images/train/fear/5539.jpg     1,2,4,5\n",
       "...      ...                                        ...         ...\n",
       "135     fear  ./data/images/images/train/fear/10993.jpg  1,2,4,5,25\n",
       "181     fear  ./data/images/images/train/fear/11442.jpg  1,2,4,5,25\n",
       "1531    fear  ./data/images/images/train/fear/21845.jpg  1,2,4,5,25\n",
       "1418    fear  ./data/images/images/train/fear/20928.jpg     1,2,4,5\n",
       "880     fear  ./data/images/images/train/fear/16754.jpg     1,2,4,5\n",
       "\n",
       "[2872 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df_modelo, test_size=0.3)\n",
    "train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6be12063-4f60-41cb-a603-01016ee67c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2872 validated image filenames belonging to 2 classes.\n",
      "Found 1231 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    zoom_range = 0.2, \n",
    "    shear_range = 0.2, \n",
    "    horizontal_flip=True, \n",
    "    rescale = 1./255\n",
    ")\n",
    "\n",
    "train_data = train_datagen.flow_from_dataframe(\n",
    "    dataframe = pd.concat([train]).sample(frac=1),\n",
    "    directory=\".\",\n",
    "    x_col = \"img\",\n",
    "    y_col = \"aus\",\n",
    "    subset = \"training\",\n",
    "    batch_size = 32,\n",
    "    seed = 42,\n",
    "    class_mode = \"categorical\",\n",
    "    target_size = (48,48)\n",
    ")\n",
    "\n",
    "test_data = train_datagen.flow_from_dataframe(\n",
    "    dataframe = pd.concat([test]).sample(frac=1),\n",
    "    directory=\".\",\n",
    "    x_col = \"img\",\n",
    "    y_col = \"aus\",\n",
    "    subset = \"training\",\n",
    "    batch_size = 32,\n",
    "    seed = 42,\n",
    "    class_mode = \"categorical\",\n",
    "    target_size = (48,48)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a6a8d0f-99a5-4b15-be3e-ff61c1014198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "10/10 [==============================] - 5s 334ms/step - loss: 0.6933 - accuracy: 0.4875 - val_loss: 0.6933 - val_accuracy: 0.4531\n",
      "Epoch 2/30\n",
      "10/10 [==============================] - 2s 240ms/step - loss: 0.6932 - accuracy: 0.5188 - val_loss: 0.6931 - val_accuracy: 0.5156\n",
      "Epoch 3/30\n",
      "10/10 [==============================] - 2s 181ms/step - loss: 0.6932 - accuracy: 0.4938 - val_loss: 0.6931 - val_accuracy: 0.5664\n",
      "Epoch 4/30\n",
      "10/10 [==============================] - 2s 206ms/step - loss: 0.6931 - accuracy: 0.5256 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 5/30\n",
      "10/10 [==============================] - 2s 172ms/step - loss: 0.6934 - accuracy: 0.5031 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 6/30\n",
      "10/10 [==============================] - 1s 144ms/step - loss: 0.6930 - accuracy: 0.5125 - val_loss: 0.6934 - val_accuracy: 0.4883\n",
      "Epoch 7/30\n",
      "10/10 [==============================] - 1s 148ms/step - loss: 0.6937 - accuracy: 0.4781 - val_loss: 0.6928 - val_accuracy: 0.5234\n",
      "Epoch 8/30\n",
      "10/10 [==============================] - 1s 154ms/step - loss: 0.6927 - accuracy: 0.5353 - val_loss: 0.6925 - val_accuracy: 0.5469\n",
      "Epoch 9/30\n",
      "10/10 [==============================] - 1s 139ms/step - loss: 0.6935 - accuracy: 0.4812 - val_loss: 0.6922 - val_accuracy: 0.5625\n",
      "Epoch 10/30\n",
      "10/10 [==============================] - 1s 132ms/step - loss: 0.6935 - accuracy: 0.4906 - val_loss: 0.6930 - val_accuracy: 0.5234\n",
      "Epoch 11/30\n",
      "10/10 [==============================] - 1s 131ms/step - loss: 0.6929 - accuracy: 0.5312 - val_loss: 0.6926 - val_accuracy: 0.5586\n",
      "Epoch 12/30\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.6933 - accuracy: 0.4875 - val_loss: 0.6932 - val_accuracy: 0.4961\n",
      "Epoch 13/30\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 0.6935 - accuracy: 0.4812 - val_loss: 0.6930 - val_accuracy: 0.5117\n",
      "Epoch 14/30\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.6934 - accuracy: 0.4750 - val_loss: 0.6930 - val_accuracy: 0.5664\n",
      "Epoch 15/30\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4961\n",
      "Epoch 16/30\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.6935 - accuracy: 0.4844 - val_loss: 0.6935 - val_accuracy: 0.4727\n",
      "Epoch 17/30\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.6929 - accuracy: 0.5219 - val_loss: 0.6938 - val_accuracy: 0.4609\n",
      "Epoch 18/30\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.6926 - accuracy: 0.5312 - val_loss: 0.6934 - val_accuracy: 0.4961\n",
      "Epoch 19/30\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.6942 - accuracy: 0.4656 - val_loss: 0.6946 - val_accuracy: 0.4492\n",
      "Epoch 20/30\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.6942 - accuracy: 0.4647 - val_loss: 0.6937 - val_accuracy: 0.4414\n",
      "Epoch 21/30\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.6932 - accuracy: 0.4656 - val_loss: 0.6931 - val_accuracy: 0.5508\n",
      "Epoch 22/30\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.6930 - accuracy: 0.5281 - val_loss: 0.6924 - val_accuracy: 0.5547\n",
      "Epoch 23/30\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.6934 - accuracy: 0.4875 - val_loss: 0.6933 - val_accuracy: 0.4922\n",
      "Epoch 24/30\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 0.6935 - accuracy: 0.4812 - val_loss: 0.6925 - val_accuracy: 0.5469\n",
      "Epoch 25/30\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 0.6933 - accuracy: 0.4875 - val_loss: 0.6930 - val_accuracy: 0.5117\n",
      "Epoch 26/30\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.6932 - accuracy: 0.4906 - val_loss: 0.6931 - val_accuracy: 0.5312\n",
      "Epoch 27/30\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.6931 - accuracy: 0.5219 - val_loss: 0.6934 - val_accuracy: 0.4492\n",
      "Epoch 28/30\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.6932 - accuracy: 0.4750 - val_loss: 0.6931 - val_accuracy: 0.5430\n",
      "Epoch 29/30\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.6932 - accuracy: 0.4906 - val_loss: 0.6929 - val_accuracy: 0.5664\n",
      "Epoch 30/30\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.6929 - accuracy: 0.5344 - val_loss: 0.6927 - val_accuracy: 0.5273\n",
      "\n",
      "Accuracy: max_acc = 53.53%\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(\n",
    "    train_data,\n",
    "    steps_per_epoch=10,\n",
    "    epochs=30,\n",
    "    validation_data=test_data,\n",
    "    validation_steps=8,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath='./results/model_aus_fear.h5',\n",
    "            save_best_only=True,\n",
    "            monitor=\"accuracy\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print()\n",
    "max_acc = max(hist.history['accuracy']) * 100\n",
    "print(f'Accuracy: {max_acc = :.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8626785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 666ms/step\n",
      "1,2,4,5: 50.94%\n"
     ]
    }
   ],
   "source": [
    "labels = np.sort(df_modelo['aus'].unique())\n",
    "img_cv = cv.imread('./data/images/images/validation/fear/21.jpg', cv.IMREAD_COLOR)\n",
    "img_keras = img_to_array(img_cv)\n",
    "img_keras = np.expand_dims(img_keras, axis=0)\n",
    "img_keras = smart_resize(img_keras, (224, 224))\n",
    "prediction = model.predict(img_keras)[0]\n",
    "mean = prediction.mean()\n",
    "std = prediction.std()\n",
    "prediction_idx = np.argmax(prediction)\n",
    "prediction_label = labels[prediction_idx]\n",
    "prediction_prob = prediction[prediction_idx]\n",
    "\n",
    "print(f'{prediction_label}: {round(prediction_prob * 100, 2)}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "3bac574fe11901c98da7470beb09037204aaaea101993e5af58b068248c4f83d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
